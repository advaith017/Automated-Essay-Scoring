{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-27T04:49:55.541258Z","iopub.execute_input":"2023-04-27T04:49:55.541729Z","iopub.status.idle":"2023-04-27T04:49:55.591444Z","shell.execute_reply.started":"2023-04-27T04:49:55.541685Z","shell.execute_reply":"2023-04-27T04:49:55.590354Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n/kaggle/input/feedback-prize-english-language-learning/train.csv\n/kaggle/input/feedback-prize-english-language-learning/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split, KFold\nimport nltk","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:49:55.593442Z","iopub.execute_input":"2023-04-27T04:49:55.593765Z","iopub.status.idle":"2023-04-27T04:50:04.339567Z","shell.execute_reply.started":"2023-04-27T04:49:55.593729Z","shell.execute_reply":"2023-04-27T04:50:04.338385Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Config:\n    vocab_size = 3000\n    embed_size = int(vocab_size ** 0.5)\n    batch_size = 32\n    epochs = 30\n    use_k_fold = True\n    target_columns = [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n    dataset_path = \"../input/feedback-prize-english-language-learning\"\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:50:04.341933Z","iopub.execute_input":"2023-04-27T04:50:04.342754Z","iopub.status.idle":"2023-04-27T04:50:04.350835Z","shell.execute_reply.started":"2023-04-27T04:50:04.342713Z","shell.execute_reply":"2023-04-27T04:50:04.349859Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(f\"{config.dataset_path}/train.csv\")\ntest = pd.read_csv(f\"{config.dataset_path}/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:50:04.352280Z","iopub.execute_input":"2023-04-27T04:50:04.353430Z","iopub.status.idle":"2023-04-27T04:50:04.569919Z","shell.execute_reply.started":"2023-04-27T04:50:04.353393Z","shell.execute_reply":"2023-04-27T04:50:04.568769Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:50:04.573815Z","iopub.execute_input":"2023-04-27T04:50:04.574124Z","iopub.status.idle":"2023-04-27T04:50:04.597920Z","shell.execute_reply.started":"2023-04-27T04:50:04.574094Z","shell.execute_reply":"2023-04-27T04:50:04.596773Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        text_id                                          full_text  cohesion  \\\n0  0016926B079C  I think that students would benefit from learn...       3.5   \n1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n3  003885A45F42  The best time in life is when you become yours...       4.5   \n4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  \n0     3.5         3.0          3.0      4.0          3.0  \n1     2.5         3.0          2.0      2.0          2.5  \n2     3.5         3.0          3.0      3.0          2.5  \n3     4.5         4.5          4.5      4.0          5.0  \n4     3.0         3.0          3.0      2.5          2.5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>When a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>The best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>Small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train[\"text\"] = train[\"full_text\"].apply(lambda sentence: \" \". join(nltk.word_tokenize(sentence.lower())))\ntest[\"text\"] = test[\"full_text\"].apply(lambda sentence: \" \". join(nltk.word_tokenize(sentence.lower())))\nvectorizor = keras.layers.TextVectorization(\n    max_tokens=config.vocab_size, \n    output_mode=\"tf-idf\", \n    ngrams=3\n)\nvectorizor.adapt(list(train[\"text\"]) + list(test[\"text\"]))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:50:04.599928Z","iopub.execute_input":"2023-04-27T04:50:04.600328Z","iopub.status.idle":"2023-04-27T04:50:28.611454Z","shell.execute_reply.started":"2023-04-27T04:50:04.600277Z","shell.execute_reply":"2023-04-27T04:50:28.610274Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    model = keras.Sequential([\n        keras.Input(shape=(), dtype=\"string\"),\n        vectorizor,\n        keras.layers.Dense(32, kernel_initializer='he_uniform', activation='sigmoid'),\n        keras.layers.Dropout(0.1),\n        keras.layers.Dense(len(config.target_columns))\n    ])\n    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[rmse])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:50:28.613398Z","iopub.execute_input":"2023-04-27T04:50:28.613803Z","iopub.status.idle":"2023-04-27T04:50:28.622078Z","shell.execute_reply.started":"2023-04-27T04:50:28.613766Z","shell.execute_reply":"2023-04-27T04:50:28.621025Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:50:28.623655Z","iopub.execute_input":"2023-04-27T04:50:28.624391Z","iopub.status.idle":"2023-04-27T04:50:28.770167Z","shell.execute_reply.started":"2023-04-27T04:50:28.624350Z","shell.execute_reply":"2023-04-27T04:50:28.769277Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text_vectorization (TextVec  (None, 3000)             1         \n torization)                                                     \n                                                                 \n dense (Dense)               (None, 32)                96032     \n                                                                 \n dropout (Dropout)           (None, 32)                0         \n                                                                 \n dense_1 (Dense)             (None, 6)                 198       \n                                                                 \n=================================================================\nTotal params: 96,231\nTrainable params: 96,230\nNon-trainable params: 1\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"keras.backend.clear_session()\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nmodels = []\nrmses = []\nfor i, (train_indices, valid_indices) in enumerate(kfold.split(train)):\n    x_train = train.iloc[train_indices][\"text\"]\n    y_train = train.iloc[train_indices][config.target_columns]\n    x_val = train.iloc[valid_indices][\"text\"]\n    y_val = train.iloc[valid_indices][config.target_columns]\n    model_path = f\"model_{i}.tf\"\n    model = get_model()\n    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    checkpoint = keras.callbacks.ModelCheckpoint(model_path, monitor=\"val_rmse\", mode=\"min\", save_best_only=True, save_weights_only=True)\n    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_rmse\", mode=\"min\", patience=5)\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['accuracy',rmse])\n    history = model.fit(\n        x_train, y_train, \n        batch_size=config.batch_size, \n        epochs=config.epochs,\n        validation_data=(x_val, y_val),\n        callbacks=[checkpoint, early_stop]\n    )\n    model.load_weights(model_path)\n    result = model.evaluate(x_val, y_val)\n    print(\"Loss:\", result[0], \"RMSE:\", result[1])\n    rmses.append(result[1])\n    models.append(model) \n    if not config.use_k_fold:\n        break\nprint(f\"Mean RMSE:{np.mean(rmses)}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:50:28.771382Z","iopub.execute_input":"2023-04-27T04:50:28.771781Z","iopub.status.idle":"2023-04-27T04:53:24.372026Z","shell.execute_reply.started":"2023-04-27T04:50:28.771753Z","shell.execute_reply":"2023-04-27T04:53:24.370859Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/30\n98/98 [==============================] - 5s 15ms/step - loss: 5.7151 - accuracy: 0.0812 - rmse: 2.3906 - val_loss: 1.9198 - val_accuracy: 0.0639 - val_rmse: 1.3856\nEpoch 2/30\n98/98 [==============================] - 1s 12ms/step - loss: 1.3283 - accuracy: 0.0956 - rmse: 1.1525 - val_loss: 0.7165 - val_accuracy: 0.0639 - val_rmse: 0.8465\nEpoch 3/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.7082 - accuracy: 0.1260 - rmse: 0.8415 - val_loss: 0.4734 - val_accuracy: 0.0651 - val_rmse: 0.6880\nEpoch 4/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.5870 - accuracy: 0.1528 - rmse: 0.7661 - val_loss: 0.4312 - val_accuracy: 0.0741 - val_rmse: 0.6567\nEpoch 5/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.5953 - accuracy: 0.1841 - rmse: 0.7716 - val_loss: 0.4094 - val_accuracy: 0.2363 - val_rmse: 0.6399\nEpoch 6/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.5515 - accuracy: 0.1992 - rmse: 0.7426 - val_loss: 0.3866 - val_accuracy: 0.1635 - val_rmse: 0.6217\nEpoch 7/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.5167 - accuracy: 0.1902 - rmse: 0.7188 - val_loss: 0.3762 - val_accuracy: 0.1264 - val_rmse: 0.6134\nEpoch 8/30\n98/98 [==============================] - 2s 17ms/step - loss: 0.5021 - accuracy: 0.1889 - rmse: 0.7086 - val_loss: 0.3476 - val_accuracy: 0.1737 - val_rmse: 0.5896\nEpoch 9/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.4605 - accuracy: 0.2004 - rmse: 0.6786 - val_loss: 0.3465 - val_accuracy: 0.1826 - val_rmse: 0.5886\nEpoch 10/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4399 - accuracy: 0.1969 - rmse: 0.6632 - val_loss: 0.3431 - val_accuracy: 0.1928 - val_rmse: 0.5858\nEpoch 11/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.4319 - accuracy: 0.2004 - rmse: 0.6572 - val_loss: 0.3378 - val_accuracy: 0.1660 - val_rmse: 0.5812\nEpoch 12/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4107 - accuracy: 0.2056 - rmse: 0.6408 - val_loss: 0.3276 - val_accuracy: 0.1967 - val_rmse: 0.5724\nEpoch 13/30\n98/98 [==============================] - 1s 11ms/step - loss: 0.4021 - accuracy: 0.2116 - rmse: 0.6341 - val_loss: 0.3335 - val_accuracy: 0.1788 - val_rmse: 0.5775\nEpoch 14/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3954 - accuracy: 0.1988 - rmse: 0.6288 - val_loss: 0.3320 - val_accuracy: 0.1852 - val_rmse: 0.5762\nEpoch 15/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3746 - accuracy: 0.2094 - rmse: 0.6121 - val_loss: 0.3325 - val_accuracy: 0.2069 - val_rmse: 0.5767\nEpoch 16/30\n98/98 [==============================] - 1s 11ms/step - loss: 0.3729 - accuracy: 0.2104 - rmse: 0.6106 - val_loss: 0.3317 - val_accuracy: 0.2018 - val_rmse: 0.5760\nEpoch 17/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3680 - accuracy: 0.2139 - rmse: 0.6066 - val_loss: 0.3335 - val_accuracy: 0.1903 - val_rmse: 0.5775\n25/25 [==============================] - 0s 9ms/step - loss: 0.3276 - accuracy: 0.1967 - rmse: 0.5724\nLoss: 0.327590674161911 RMSE: 0.19667944312095642\nEpoch 1/30\n98/98 [==============================] - 2s 14ms/step - loss: 5.4300 - accuracy: 0.0748 - rmse: 2.3302 - val_loss: 1.0902 - val_accuracy: 0.0703 - val_rmse: 1.0441\nEpoch 2/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.8075 - accuracy: 0.1256 - rmse: 0.8986 - val_loss: 0.4672 - val_accuracy: 0.2174 - val_rmse: 0.6835\nEpoch 3/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.5894 - accuracy: 0.1943 - rmse: 0.7677 - val_loss: 0.4334 - val_accuracy: 0.1905 - val_rmse: 0.6583\nEpoch 4/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.5639 - accuracy: 0.2151 - rmse: 0.7509 - val_loss: 0.3950 - val_accuracy: 0.3261 - val_rmse: 0.6285\nEpoch 5/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.5116 - accuracy: 0.1889 - rmse: 0.7153 - val_loss: 0.3746 - val_accuracy: 0.2327 - val_rmse: 0.6121\nEpoch 6/30\n98/98 [==============================] - 1s 14ms/step - loss: 0.4915 - accuracy: 0.2039 - rmse: 0.7011 - val_loss: 0.3524 - val_accuracy: 0.1969 - val_rmse: 0.5937\nEpoch 7/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4711 - accuracy: 0.2068 - rmse: 0.6863 - val_loss: 0.3489 - val_accuracy: 0.1867 - val_rmse: 0.5906\nEpoch 8/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4464 - accuracy: 0.1959 - rmse: 0.6681 - val_loss: 0.3450 - val_accuracy: 0.2391 - val_rmse: 0.5874\nEpoch 9/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4407 - accuracy: 0.2004 - rmse: 0.6638 - val_loss: 0.3392 - val_accuracy: 0.2506 - val_rmse: 0.5824\nEpoch 10/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4146 - accuracy: 0.2026 - rmse: 0.6439 - val_loss: 0.3405 - val_accuracy: 0.2238 - val_rmse: 0.5835\nEpoch 11/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4053 - accuracy: 0.2180 - rmse: 0.6367 - val_loss: 0.3401 - val_accuracy: 0.2097 - val_rmse: 0.5832\nEpoch 12/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3911 - accuracy: 0.2387 - rmse: 0.6254 - val_loss: 0.3225 - val_accuracy: 0.2442 - val_rmse: 0.5679\nEpoch 13/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3764 - accuracy: 0.2394 - rmse: 0.6135 - val_loss: 0.3331 - val_accuracy: 0.2110 - val_rmse: 0.5771\nEpoch 14/30\n98/98 [==============================] - 2s 18ms/step - loss: 0.3644 - accuracy: 0.2387 - rmse: 0.6036 - val_loss: 0.3204 - val_accuracy: 0.2660 - val_rmse: 0.5660\nEpoch 15/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3478 - accuracy: 0.2490 - rmse: 0.5897 - val_loss: 0.3195 - val_accuracy: 0.2430 - val_rmse: 0.5652\nEpoch 16/30\n98/98 [==============================] - 1s 14ms/step - loss: 0.3338 - accuracy: 0.2474 - rmse: 0.5777 - val_loss: 0.3265 - val_accuracy: 0.2775 - val_rmse: 0.5714\nEpoch 17/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3273 - accuracy: 0.2649 - rmse: 0.5721 - val_loss: 0.3191 - val_accuracy: 0.2877 - val_rmse: 0.5649\nEpoch 18/30\n98/98 [==============================] - 1s 11ms/step - loss: 0.3138 - accuracy: 0.2601 - rmse: 0.5602 - val_loss: 0.3209 - val_accuracy: 0.2532 - val_rmse: 0.5664\nEpoch 19/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3104 - accuracy: 0.2576 - rmse: 0.5571 - val_loss: 0.3470 - val_accuracy: 0.2353 - val_rmse: 0.5891\nEpoch 20/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3011 - accuracy: 0.2665 - rmse: 0.5487 - val_loss: 0.3184 - val_accuracy: 0.3005 - val_rmse: 0.5643\nEpoch 21/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.2988 - accuracy: 0.2723 - rmse: 0.5466 - val_loss: 0.3206 - val_accuracy: 0.2737 - val_rmse: 0.5662\nEpoch 22/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.2950 - accuracy: 0.2755 - rmse: 0.5431 - val_loss: 0.3319 - val_accuracy: 0.2059 - val_rmse: 0.5761\nEpoch 23/30\n98/98 [==============================] - 1s 14ms/step - loss: 0.2871 - accuracy: 0.2665 - rmse: 0.5358 - val_loss: 0.3255 - val_accuracy: 0.2673 - val_rmse: 0.5705\nEpoch 24/30\n98/98 [==============================] - 2s 17ms/step - loss: 0.2743 - accuracy: 0.2790 - rmse: 0.5238 - val_loss: 0.3224 - val_accuracy: 0.2481 - val_rmse: 0.5678\nEpoch 25/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.2696 - accuracy: 0.2720 - rmse: 0.5193 - val_loss: 0.3228 - val_accuracy: 0.2583 - val_rmse: 0.5682\n25/25 [==============================] - 0s 9ms/step - loss: 0.3184 - accuracy: 0.3005 - rmse: 0.5643\nLoss: 0.3184068500995636 RMSE: 0.30051150918006897\nEpoch 1/30\n98/98 [==============================] - 2s 15ms/step - loss: 4.8610 - accuracy: 0.0962 - rmse: 2.2048 - val_loss: 1.6629 - val_accuracy: 0.0831 - val_rmse: 1.2895\nEpoch 2/30\n98/98 [==============================] - 1s 13ms/step - loss: 1.1319 - accuracy: 0.1067 - rmse: 1.0639 - val_loss: 0.5877 - val_accuracy: 0.0831 - val_rmse: 0.7666\nEpoch 3/30\n98/98 [==============================] - 1s 14ms/step - loss: 0.6516 - accuracy: 0.1592 - rmse: 0.8072 - val_loss: 0.4367 - val_accuracy: 0.2404 - val_rmse: 0.6609\nEpoch 4/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.5837 - accuracy: 0.1726 - rmse: 0.7640 - val_loss: 0.4116 - val_accuracy: 0.2519 - val_rmse: 0.6416\nEpoch 5/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.5827 - accuracy: 0.1633 - rmse: 0.7633 - val_loss: 0.3915 - val_accuracy: 0.2749 - val_rmse: 0.6257\nEpoch 6/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.5367 - accuracy: 0.1921 - rmse: 0.7326 - val_loss: 0.3829 - val_accuracy: 0.2391 - val_rmse: 0.6188\nEpoch 7/30\n98/98 [==============================] - 1s 14ms/step - loss: 0.5107 - accuracy: 0.1854 - rmse: 0.7146 - val_loss: 0.3562 - val_accuracy: 0.2532 - val_rmse: 0.5969\nEpoch 8/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.4964 - accuracy: 0.1991 - rmse: 0.7046 - val_loss: 0.3508 - val_accuracy: 0.2455 - val_rmse: 0.5923\nEpoch 9/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.4764 - accuracy: 0.2049 - rmse: 0.6902 - val_loss: 0.3397 - val_accuracy: 0.2596 - val_rmse: 0.5828\nEpoch 10/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4518 - accuracy: 0.1905 - rmse: 0.6722 - val_loss: 0.3391 - val_accuracy: 0.2558 - val_rmse: 0.5823\nEpoch 11/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4410 - accuracy: 0.2081 - rmse: 0.6640 - val_loss: 0.3313 - val_accuracy: 0.2519 - val_rmse: 0.5756\nEpoch 12/30\n98/98 [==============================] - 2s 18ms/step - loss: 0.4255 - accuracy: 0.1962 - rmse: 0.6523 - val_loss: 0.3443 - val_accuracy: 0.1355 - val_rmse: 0.5867\nEpoch 13/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4215 - accuracy: 0.2039 - rmse: 0.6493 - val_loss: 0.3344 - val_accuracy: 0.1739 - val_rmse: 0.5783\nEpoch 14/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4038 - accuracy: 0.2141 - rmse: 0.6354 - val_loss: 0.3332 - val_accuracy: 0.2519 - val_rmse: 0.5772\nEpoch 15/30\n98/98 [==============================] - 1s 14ms/step - loss: 0.3895 - accuracy: 0.2036 - rmse: 0.6241 - val_loss: 0.3504 - val_accuracy: 0.2379 - val_rmse: 0.5920\nEpoch 16/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3960 - accuracy: 0.2419 - rmse: 0.6293 - val_loss: 0.3251 - val_accuracy: 0.2621 - val_rmse: 0.5702\nEpoch 17/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3679 - accuracy: 0.2343 - rmse: 0.6066 - val_loss: 0.3245 - val_accuracy: 0.2481 - val_rmse: 0.5697\nEpoch 18/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3623 - accuracy: 0.2282 - rmse: 0.6019 - val_loss: 0.3339 - val_accuracy: 0.2532 - val_rmse: 0.5778\nEpoch 19/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3607 - accuracy: 0.2410 - rmse: 0.6006 - val_loss: 0.3289 - val_accuracy: 0.2519 - val_rmse: 0.5735\nEpoch 20/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3534 - accuracy: 0.2253 - rmse: 0.5945 - val_loss: 0.3351 - val_accuracy: 0.2455 - val_rmse: 0.5789\nEpoch 21/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3430 - accuracy: 0.2509 - rmse: 0.5856 - val_loss: 0.3249 - val_accuracy: 0.2417 - val_rmse: 0.5700\nEpoch 22/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3378 - accuracy: 0.2570 - rmse: 0.5812 - val_loss: 0.3262 - val_accuracy: 0.1969 - val_rmse: 0.5711\n25/25 [==============================] - 0s 13ms/step - loss: 0.3245 - accuracy: 0.2481 - rmse: 0.5697\nLoss: 0.3245217502117157 RMSE: 0.24808184802532196\nEpoch 1/30\n98/98 [==============================] - 2s 14ms/step - loss: 4.3148 - accuracy: 0.3778 - rmse: 2.0772 - val_loss: 1.0573 - val_accuracy: 0.2148 - val_rmse: 1.0283\nEpoch 2/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.7434 - accuracy: 0.2135 - rmse: 0.8622 - val_loss: 0.4508 - val_accuracy: 0.2136 - val_rmse: 0.6714\nEpoch 3/30\n98/98 [==============================] - 2s 17ms/step - loss: 0.5350 - accuracy: 0.2026 - rmse: 0.7315 - val_loss: 0.4286 - val_accuracy: 0.2110 - val_rmse: 0.6547\nEpoch 4/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.5338 - accuracy: 0.1943 - rmse: 0.7306 - val_loss: 0.4275 - val_accuracy: 0.1432 - val_rmse: 0.6538\nEpoch 5/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.5213 - accuracy: 0.2058 - rmse: 0.7220 - val_loss: 0.4157 - val_accuracy: 0.2072 - val_rmse: 0.6448\nEpoch 6/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.4980 - accuracy: 0.2061 - rmse: 0.7057 - val_loss: 0.4043 - val_accuracy: 0.1944 - val_rmse: 0.6359\nEpoch 7/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4788 - accuracy: 0.2058 - rmse: 0.6920 - val_loss: 0.3841 - val_accuracy: 0.2609 - val_rmse: 0.6197\nEpoch 8/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4495 - accuracy: 0.2234 - rmse: 0.6705 - val_loss: 0.3623 - val_accuracy: 0.2928 - val_rmse: 0.6019\nEpoch 9/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.4193 - accuracy: 0.2154 - rmse: 0.6475 - val_loss: 0.3568 - val_accuracy: 0.3082 - val_rmse: 0.5973\nEpoch 10/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3990 - accuracy: 0.2323 - rmse: 0.6317 - val_loss: 0.3523 - val_accuracy: 0.2072 - val_rmse: 0.5935\nEpoch 11/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3957 - accuracy: 0.2183 - rmse: 0.6290 - val_loss: 0.3525 - val_accuracy: 0.1586 - val_rmse: 0.5937\nEpoch 12/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3742 - accuracy: 0.2100 - rmse: 0.6118 - val_loss: 0.3473 - val_accuracy: 0.2033 - val_rmse: 0.5893\nEpoch 13/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3723 - accuracy: 0.2314 - rmse: 0.6101 - val_loss: 0.3481 - val_accuracy: 0.2289 - val_rmse: 0.5900\nEpoch 14/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3514 - accuracy: 0.2196 - rmse: 0.5927 - val_loss: 0.3448 - val_accuracy: 0.1880 - val_rmse: 0.5872\nEpoch 15/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3414 - accuracy: 0.2205 - rmse: 0.5843 - val_loss: 0.3432 - val_accuracy: 0.3120 - val_rmse: 0.5858\nEpoch 16/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3384 - accuracy: 0.2256 - rmse: 0.5817 - val_loss: 0.3480 - val_accuracy: 0.2379 - val_rmse: 0.5899\nEpoch 17/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3307 - accuracy: 0.2205 - rmse: 0.5751 - val_loss: 0.3385 - val_accuracy: 0.2276 - val_rmse: 0.5818\nEpoch 18/30\n98/98 [==============================] - 1s 11ms/step - loss: 0.3176 - accuracy: 0.2218 - rmse: 0.5635 - val_loss: 0.3489 - val_accuracy: 0.2084 - val_rmse: 0.5907\nEpoch 19/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3304 - accuracy: 0.2336 - rmse: 0.5748 - val_loss: 0.3381 - val_accuracy: 0.2097 - val_rmse: 0.5815\nEpoch 20/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3142 - accuracy: 0.2234 - rmse: 0.5605 - val_loss: 0.3425 - val_accuracy: 0.1675 - val_rmse: 0.5852\nEpoch 21/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3007 - accuracy: 0.2307 - rmse: 0.5483 - val_loss: 0.3434 - val_accuracy: 0.3184 - val_rmse: 0.5860\nEpoch 22/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.2923 - accuracy: 0.2407 - rmse: 0.5406 - val_loss: 0.3397 - val_accuracy: 0.1905 - val_rmse: 0.5829\nEpoch 23/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.2919 - accuracy: 0.2422 - rmse: 0.5403 - val_loss: 0.3395 - val_accuracy: 0.2033 - val_rmse: 0.5827\nEpoch 24/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.2899 - accuracy: 0.2422 - rmse: 0.5384 - val_loss: 0.3389 - val_accuracy: 0.2417 - val_rmse: 0.5822\n25/25 [==============================] - 0s 9ms/step - loss: 0.3381 - accuracy: 0.2097 - rmse: 0.5815\nLoss: 0.3381160795688629 RMSE: 0.20971867442131042\nEpoch 1/30\n98/98 [==============================] - 2s 15ms/step - loss: 5.3802 - accuracy: 0.0933 - rmse: 2.3195 - val_loss: 1.7134 - val_accuracy: 0.0729 - val_rmse: 1.3090\nEpoch 2/30\n98/98 [==============================] - 1s 13ms/step - loss: 1.0196 - accuracy: 0.0914 - rmse: 1.0098 - val_loss: 0.5200 - val_accuracy: 0.0742 - val_rmse: 0.7211\nEpoch 3/30\n98/98 [==============================] - 2s 17ms/step - loss: 0.5899 - accuracy: 0.1250 - rmse: 0.7680 - val_loss: 0.4095 - val_accuracy: 0.0985 - val_rmse: 0.6399\nEpoch 4/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.5382 - accuracy: 0.1841 - rmse: 0.7336 - val_loss: 0.3880 - val_accuracy: 0.1304 - val_rmse: 0.6229\nEpoch 5/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.5152 - accuracy: 0.1895 - rmse: 0.7178 - val_loss: 0.3716 - val_accuracy: 0.1419 - val_rmse: 0.6096\nEpoch 6/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4930 - accuracy: 0.1937 - rmse: 0.7022 - val_loss: 0.3705 - val_accuracy: 0.1266 - val_rmse: 0.6087\nEpoch 7/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4569 - accuracy: 0.1844 - rmse: 0.6759 - val_loss: 0.3486 - val_accuracy: 0.1483 - val_rmse: 0.5904\nEpoch 8/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.4264 - accuracy: 0.2071 - rmse: 0.6530 - val_loss: 0.3424 - val_accuracy: 0.1598 - val_rmse: 0.5852\nEpoch 9/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4060 - accuracy: 0.2144 - rmse: 0.6371 - val_loss: 0.3318 - val_accuracy: 0.1573 - val_rmse: 0.5760\nEpoch 10/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.4001 - accuracy: 0.2061 - rmse: 0.6325 - val_loss: 0.3320 - val_accuracy: 0.1496 - val_rmse: 0.5762\nEpoch 11/30\n98/98 [==============================] - 1s 11ms/step - loss: 0.4068 - accuracy: 0.2212 - rmse: 0.6378 - val_loss: 0.3474 - val_accuracy: 0.3210 - val_rmse: 0.5894\nEpoch 12/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3729 - accuracy: 0.2349 - rmse: 0.6106 - val_loss: 0.3234 - val_accuracy: 0.1969 - val_rmse: 0.5687\nEpoch 13/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3547 - accuracy: 0.2416 - rmse: 0.5956 - val_loss: 0.3239 - val_accuracy: 0.1547 - val_rmse: 0.5691\nEpoch 14/30\n98/98 [==============================] - 1s 11ms/step - loss: 0.3533 - accuracy: 0.2266 - rmse: 0.5944 - val_loss: 0.3321 - val_accuracy: 0.2737 - val_rmse: 0.5763\nEpoch 15/30\n98/98 [==============================] - 1s 13ms/step - loss: 0.3374 - accuracy: 0.2288 - rmse: 0.5808 - val_loss: 0.3259 - val_accuracy: 0.1944 - val_rmse: 0.5708\nEpoch 16/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3327 - accuracy: 0.2368 - rmse: 0.5768 - val_loss: 0.3343 - val_accuracy: 0.2097 - val_rmse: 0.5782\nEpoch 17/30\n98/98 [==============================] - 1s 12ms/step - loss: 0.3186 - accuracy: 0.2649 - rmse: 0.5644 - val_loss: 0.3393 - val_accuracy: 0.2251 - val_rmse: 0.5825\n25/25 [==============================] - 0s 9ms/step - loss: 0.3234 - accuracy: 0.1969 - rmse: 0.5687\nLoss: 0.32338231801986694 RMSE: 0.19693094491958618\nMean RMSE:0.2303844839334488\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = []\nfor model in models:\n    preds.append(model.predict(test[\"text\"]))\npred = np.mean(preds, axis=0)\nsubmission = pd.DataFrame({\n    \"text_id\": test[\"text_id\"]\n})\nfor i in range(len(config.target_columns)):\n    column = config.target_columns[i]\n    submission[column] = pred[:, i]\npred = np.mean(preds, axis=0)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:53:24.373742Z","iopub.execute_input":"2023-04-27T04:53:24.374076Z","iopub.status.idle":"2023-04-27T04:53:25.157217Z","shell.execute_reply.started":"2023-04-27T04:53:24.374040Z","shell.execute_reply":"2023-04-27T04:53:25.156247Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 136ms/step\n1/1 [==============================] - 0s 105ms/step\n1/1 [==============================] - 0s 104ms/step\n1/1 [==============================] - 0s 108ms/step\n1/1 [==============================] - 0s 105ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = []\nfor model in models:\n    preds.append(model.predict(train[\"full_text\"]))\npred = np.mean(preds, axis=0)\nsubmission = pd.DataFrame({\n    \"text_id\": train[\"text_id\"]\n})\nfor i in range(len(config.target_columns)):\n    column = config.target_columns[i]\n    submission[column] = pred[:, i]\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:08:02.296905Z","iopub.execute_input":"2023-04-27T05:08:02.297894Z","iopub.status.idle":"2023-04-27T05:08:08.695297Z","shell.execute_reply.started":"2023-04-27T05:08:02.297857Z","shell.execute_reply":"2023-04-27T05:08:08.694247Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"123/123 [==============================] - 1s 9ms/step\n123/123 [==============================] - 1s 8ms/step\n123/123 [==============================] - 1s 8ms/step\n123/123 [==============================] - 1s 8ms/step\n123/123 [==============================] - 1s 8ms/step\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"           text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0     0016926B079C  3.299237  3.144557    3.307819     3.196914  3.223613   \n1     0022683E9EA5  2.732351  2.547126    2.743189     2.500007  2.399894   \n2     00299B378633  2.856598  2.775280    3.012170     2.837618  2.754196   \n3     003885A45F42  3.276055  3.207409    3.325592     3.306266  3.302924   \n4     0049B1DF5CCC  2.918462  2.857034    3.127614     2.958375  2.805789   \n...            ...       ...       ...         ...          ...       ...   \n3906  FFD29828A873  3.138339  3.079617    3.276016     3.163969  3.089429   \n3907  FFD9A83B0849  3.522869  3.429275    3.518063     3.520644  3.471041   \n3908  FFDC4011AC9C  3.029586  2.949958    3.233208     3.104231  3.103633   \n3909  FFE16D704B16  3.515909  3.379911    3.512368     3.486501  3.438157   \n3910  FFED00D6E0BD  3.135639  2.829186    3.157614     2.917941  2.876812   \n\n      conventions  \n0        3.123274  \n1        2.413464  \n2        2.802880  \n3        3.209436  \n4        2.775101  \n...           ...  \n3906     3.028326  \n3907     3.503082  \n3908     2.933509  \n3909     3.464025  \n3910     2.856395  \n\n[3911 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>3.299237</td>\n      <td>3.144557</td>\n      <td>3.307819</td>\n      <td>3.196914</td>\n      <td>3.223613</td>\n      <td>3.123274</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>2.732351</td>\n      <td>2.547126</td>\n      <td>2.743189</td>\n      <td>2.500007</td>\n      <td>2.399894</td>\n      <td>2.413464</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>2.856598</td>\n      <td>2.775280</td>\n      <td>3.012170</td>\n      <td>2.837618</td>\n      <td>2.754196</td>\n      <td>2.802880</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>3.276055</td>\n      <td>3.207409</td>\n      <td>3.325592</td>\n      <td>3.306266</td>\n      <td>3.302924</td>\n      <td>3.209436</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>2.918462</td>\n      <td>2.857034</td>\n      <td>3.127614</td>\n      <td>2.958375</td>\n      <td>2.805789</td>\n      <td>2.775101</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3906</th>\n      <td>FFD29828A873</td>\n      <td>3.138339</td>\n      <td>3.079617</td>\n      <td>3.276016</td>\n      <td>3.163969</td>\n      <td>3.089429</td>\n      <td>3.028326</td>\n    </tr>\n    <tr>\n      <th>3907</th>\n      <td>FFD9A83B0849</td>\n      <td>3.522869</td>\n      <td>3.429275</td>\n      <td>3.518063</td>\n      <td>3.520644</td>\n      <td>3.471041</td>\n      <td>3.503082</td>\n    </tr>\n    <tr>\n      <th>3908</th>\n      <td>FFDC4011AC9C</td>\n      <td>3.029586</td>\n      <td>2.949958</td>\n      <td>3.233208</td>\n      <td>3.104231</td>\n      <td>3.103633</td>\n      <td>2.933509</td>\n    </tr>\n    <tr>\n      <th>3909</th>\n      <td>FFE16D704B16</td>\n      <td>3.515909</td>\n      <td>3.379911</td>\n      <td>3.512368</td>\n      <td>3.486501</td>\n      <td>3.438157</td>\n      <td>3.464025</td>\n    </tr>\n    <tr>\n      <th>3910</th>\n      <td>FFED00D6E0BD</td>\n      <td>3.135639</td>\n      <td>2.829186</td>\n      <td>3.157614</td>\n      <td>2.917941</td>\n      <td>2.876812</td>\n      <td>2.856395</td>\n    </tr>\n  </tbody>\n</table>\n<p>3911 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_squared_error\n\ny_true=list(train['cohesion'])\ny_pred=list(submission['cohesion'])\nmean_squared_error(y_true,y_pred)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:38:55.110658Z","iopub.execute_input":"2023-04-27T05:38:55.111609Z","iopub.status.idle":"2023-04-27T05:38:55.121777Z","shell.execute_reply.started":"2023-04-27T05:38:55.111563Z","shell.execute_reply":"2023-04-27T05:38:55.120630Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0.3884705593445148"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}